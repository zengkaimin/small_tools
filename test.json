{'rule': {'name': 'starred_to_cubox', 'matchesToday': '10', 'matchesTotal': '10'
    }, 'items': [
        {'crawlTimeMsec': '1647992592378', 'timestampUsec': '1647992592378140', 'id': 'tag:google.com,
            2005:reader/item/000000078c6eb670', 'categories': ['user/1005323937/state/com.google/reading-list', 'user/1005323937/state/com.google/read', 'user/1005323937/state/com.google/starred'
            ], 'title': 'Links (57) & AI safety special', 'published': 1647907200, 'updated': 0, 'canonical': [
                {'href': 'https: //nintil.com/links-57/'}], 'alternate': [{'href': 'https://nintil.com/links-57/', 'type': 'text/html'}], 'summary': {'direction': 'ltr', 'content': '<p>Not that many links this months, but there\'s an special on AI safety at the end</p> \n<p>Top forecasters vs <a href="https://forum.effectivealtruism.org/posts/qZqvBLvR5hX9sEkjR/comparing-top-forecasters-and-domain-experts">domain</a> experts</p> \n<p>An introduction to high-speed (i.e. hypersonic and supersonic) <a href="https://www.primemoverslab.com/ideas/high-speed-flight">flight</a></p> \n<p>Givewell (Labs, which later became <a href="https://www.openphilanthropy.org/blog/meta-research">OpenPhilantrophy</a>) has an old <a href="https://blog.givewell.org/2013/06/06/meta-research-update/">report</a> on meta-research (as in science of science or meta-science). They started by taking to <em>the leads we had – contacts at <a href="https://blog.givewell.org/2012/09/27/us-cochrane-center-uscc-gets-our-first-quick-grant-recommendation/">Cochrane</a> as well as individuals suggested by John Ioannidis – and get referrals from them to other people he should be speaking with.</em>. Whereas I just read a bunch of papers and didn\'t talk to anyone. It seems like we found the same sort of issues. But I couldn\'t find the final report to see if we came to different conclusions. I could find <a href="https://www.openphilanthropy.org/early-open-philanthropy-thinking-science-funding">this</a>. It\'s good in that it seems they came to the same rough picture that I came to. It\'s bad in that they did so 7 years ago, and not that much has changed. In the interim, OpenPhil\'s investigations led to funding some interesting science, including Ed Boyden\'s work in <a href="https://twitter.com/eboyden3/status/757232074407481346">expansion microscopy</a>.</p> \n<p>I came across a <a href="https://arcade.stanford.edu/rofl/deceit-desire-and-literature-professor-why-girardians-exist">critique</a> of Peter Thiel\'s favorite philosopher, Rene Girard.</p> \n<p><a href="https://elifesciences.org/articles/28434">Study</a> fails to replicate another study (as part of the Reproducibility in Cancer Biology project) showing that a certain kind of bacteria is implicated in colon cancer. Naively, this would lead one to discard that hypothesis. But the paper is wiser than that: One has to evaluate the body of evidence as a whole. My take after reading that is that we need some more managed science: Taking a series of half-answered unclear questions, getting various groups to fund well powered and designed experiments (including replications of each) and seeing what can be said about this question. I might follow up with a post about this.</p> \n<p>DeepMind\'s <em>Advancing mathematics by guiding human intuition with AI</em>, <a href="https://www.youtube.com/watch?v=UPCI1-ZvwOg">explained</a> by Tim Nguyen.</p> \n<p>NIHR (UK) is launching their own \'<a href="https://www.nihr.ac.uk/blog/a-faster-way-to-accelerate-innovation/30079">Fast Grants</a>\'</p> \n<p>Science, <a href="https://kill-the-newsletter.com/alternates/q8cyp2xqeelufuvb.html">philantrophy</a>, and tech (and crypto)</p> \n<p>The potential of ML for hardware <a href="https://www.youtube.com/watch?v=FraDFZ2t__A">design</a></p> \n<p>Supercharging biomedical science at the <a href="https://www.dayoneproject.org/post/supercharging-biomedical-science-at-the-national-institutes-of-health">National Institutes of Health</a></p> \n<p>AI-powered drug discovery is also AI-powered <a href="https://www.nature.com/articles/s42256-022-00465-9?fbclid=IwAR11_V1cd9SUxEvUfwrWMA7TUcroyYIY1nBDUL3KaS-8B4rG5MIqZCmjm0M">bioweapon</a> discovery</p> \n<p>To prove that its funding system is fucked up, NIH rejects grant because of a <a href="https://twitter.com/ns_whit/status/1499887361894801415">slight</a> margin issue</p> \n<p>10% of people have an accessory <a href="https://en.wikipedia.org/wiki/Accessory_spleen">spleen</a></p> \n<p><a href="https://en.wikipedia.org/wiki/List_of_unusual_biological_names">List of unusual biological names</a>, featuring proteins like Lunatic Fringe or MAP kinase kinase kinase kinase</p> \n<h1>AI Safety special</h1> \n<p>Haven\'t looked into this in a very long time. I got curious after talking to some of the people that work in the area so I thought it was time to see what\'s going on there these days.  Here are some things I\'ve read this month that fall under the broad umbrella of designing AI systems that are safe (by various definitions of safe). They are all introductory material/general explainers. My initial impression of the field (at least as of <a href="https://nintil.com/on-the-possible-futility-of-friendly-ai-research/">2016</a>; though the end of the post notest that at some point I became more optimistic) was that they were trying to design something that seemed like <em>provably</em> safe AI systems which is hopeless (This was perhaps a misunderstanding on my part on what the early AI safety people were trying to do, but <em>Agents Foundations</em>, which was deemed a failure, seems to be literally this). Now the field seems to have merged, or at least taken inspiration from, the explainable ML community and AI safety is nowadays an eminently empirical field centered around understanding the kinds of models, like Transformers, that seem promising, and trying to devise new ways to train them that lead to desired behaviours, for example <a href="https://www.alignmentforum.org/posts/k7oxdbNaGATZbtEg3/redwood-research-s-current-project">trying to get language models to output benign completions to a given prompt</a>. There\'s more tool building, more concrete tractable problems and less theorising about arbitrarily general intelligent systems.</p> \n<ul> \n<li><a href="https://sjbyrnes.com/agi.html">Steve Byrnes\' essays</a></li> \n<li>Scott Alexander on the difficulty of predicting when we\'ll get <a href="https://astralcodexten.substack.com/p/biological-anchors-a-trick-that-might?s=r">AGI</a></li> \n<li>Richard Ngo\'s <a href="https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ">AGI safety from first principles</a></li> \n<li>Paul Christiano: <a href="https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment">Current work</a> in AI alignment (And <a href="https://www.alignmentforum.org/posts/Djs38EWYZG8o7JMWY/paul-s-research-agenda-faq">research agenda</a>)</li> \n<li>Why AI alignment could be hard with <a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">modern</a> deep learning</li> \n<li>Attempted Gears Analysis of AGI intervention Discussion With <a href="https://www.lesswrong.com/posts/xHnuX42WNZ9hq53bz/attempted-gears-analysis-of-agi-intervention-discussion-with-1">Eliezer</a></li> \n<li>AI Alignment and <a href="https://www.vincentweisser.com/ai-alignment">Safety</a></li> \n</ul> \n<p>I think the field has evolved positively since I last checked on it, and is now less of a weird niche where smart people go get high on math and more in dialogue with leading edge ML research where code (and readable essays) is the main output.</p> \n<p>Let me know if there\'s anything else that belongs in this section! -&gt; artir [at] nintil [dot] com</p>'}, 'author': '', 'likingUsers': [], 'comments': [], 'commentsNum': -1, 'annotations': [], 'origin': {'streamId': 'feed/https://nintil.com/rss.xml', 'title': 'Nintil', 'htmlUrl': 'https://nintil.com/'}}]}